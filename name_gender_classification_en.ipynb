{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>English names Gender Classification<center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook english names gender classification is implemented. The classification models are : <br>\n",
    "1. Recurrent Neural Network - LSTM \n",
    "2. Multilayer Perceptron - MLP\n",
    "\n",
    "The implemetation is in tensorflow and all necessary methods are implemeted in this notebook. Therefore it can easily be run on [colab](https://colab.research.google.com/notebooks/welcome.ipynb#recent=true) platform to test run and see the results  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sys, os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"Data/English/train_eng.csv\"\n",
    "test_data_path = \"Data/English/test_eng.csv\"\n",
    "\n",
    "# Load data\n",
    "p_train_data = pd.read_csv(data_path)\n",
    "p_test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "# Convert data to numpy arrays\n",
    "train = p_train_data.values\n",
    "test = p_test_data.values\n",
    "\n",
    "train = np.stack(sorted(list(train), key=lambda x: len(x[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set training & models parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_embedding_size = 5\n",
    "lstm_hidden_size = 5\n",
    "epochs = 50\n",
    "minibatch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data, max_len):\n",
    "    unique = list(set(\"\".join(data[:,0])))\n",
    "    unique.sort()\n",
    "    vocab = dict(zip(unique, range(1,len(unique)+1))) # start from 1 for zero padding\n",
    "\n",
    "    classes = list(set(data[:,1]))\n",
    "    classes.sort()\n",
    "    class_map = dict(zip(classes, range(len(unique))))\n",
    "\n",
    "    names = list(data[:,0])\n",
    "    labels = list(data[:,1])\n",
    "\n",
    "    def transform_name(name):\n",
    "        point = np.zeros((1, max_len), dtype=int)\n",
    "        name_mapped = np.array(list(map(lambda l: vocab[l], name)))\n",
    "        point[0,0: len(name_mapped)] = name_mapped\n",
    "        return point\n",
    "\n",
    "    transform_label = lambda lbl: np.array([[class_map[lbl]]])\n",
    "\n",
    "    names = list(map(transform_name, names))\n",
    "    labels = list(map(transform_label, labels))\n",
    "\n",
    "    names = np.concatenate(names, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "    return names, labels, vocab\n",
    "\n",
    "def get_minibatches(names, labels, mb_size):\n",
    "    batches = []\n",
    "    position = 0\n",
    "    \n",
    "    while position + mb_size < len(labels):\n",
    "        batches.append((names[position: position + mb_size], labels[position: position + mb_size]))\n",
    "        position += mb_size\n",
    "\n",
    "    batches.append((names[position:], labels[position:]))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM(emb_size, vocab_size, lstm_hidden_size, T, learning_rate=0.001):\n",
    "    pad_vector = tf.zeros(shape=(1, emb_size), dtype=tf.float32, name=\"zero_padding\")\n",
    "    symbol_embedding = tf.get_variable('symbol_embeddings', shape=(vocab_size, emb_size), dtype=tf.float32)\n",
    "\n",
    "    symbol_embedding = tf.concat([pad_vector, symbol_embedding], axis=0)\n",
    "\n",
    "    input_ = tf.placeholder(shape=[None, T], dtype=tf.int32)\n",
    "    labels_ = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "\n",
    "    embedded = tf.nn.embedding_lookup(symbol_embedding, input_)\n",
    "\n",
    "    lstm = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(cell=lstm, inputs=embedded, dtype=tf.float32)\n",
    "    output = outputs[:, -1, :]\n",
    "    logits = tf.keras.layers.Dense(1)(output)\n",
    "\n",
    "    classify = tf.nn.sigmoid(logits)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels_), axis=0)\n",
    "\n",
    "    train = tf.contrib.opt.LazyAdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    print(\"trainable parameters:\", np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()]))\n",
    "\n",
    "    return {\n",
    "        'train': train,\n",
    "        'input': input_,\n",
    "        'labels': labels_,\n",
    "        'loss': loss,\n",
    "        'classify': classify\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_LSTM(tf_session, tf_loss, tf_classify, data, labels):\n",
    "    \"\"\"\n",
    "    Evaluate loss and accuracy on a single minibatch\n",
    "    :param tf_session: current opened session\n",
    "    :param tf_loss: tensor for calculating loss\n",
    "    :param tf_classify: tensor for calculating sigmoid activations\n",
    "    :param data: data from the current batch\n",
    "    :param labels: labels from the current batch\n",
    "    :return: loss_value, accuracy_value\n",
    "    \"\"\"\n",
    "\n",
    "    loss_val, predict = tf_session.run([tf_loss, tf_classify], {\n",
    "        input_: data,\n",
    "        labels_: labels\n",
    "    })\n",
    "    acc_val = accuracy_score(labels, np.where(predict > 0.5, 1, 0))\n",
    "\n",
    "    return loss_val, acc_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify LSTM Specific training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable parameters: 486\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "max_len = p_train_data['Name'].str.len().max()\n",
    "train_data, train_labels, voc = transform_data(train, max_len)\n",
    "test_data, test_labels, _ = transform_data(test, max_len)\n",
    "batches = get_minibatches(train_data, train_labels, minibatch_size)\n",
    "terminals = create_LSTM(letter_embedding_size, len(voc), lstm_hidden_size, max_len)\n",
    "\n",
    "train_ = terminals['train']\n",
    "input_ = terminals['input']\n",
    "labels_ = terminals['labels']\n",
    "loss_ = terminals['loss']\n",
    "classify_ = terminals['classify']\n",
    "\n",
    "pl_loss = np.zeros((epochs,2))\n",
    "pl_acc = np.zeros((epochs,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluating LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train loss 0.5709, train acc 0.7132, test loss 0.5705, test accuracy 0.7134\n",
      "Epoch 2, train loss 0.5325, train acc 0.7393, test loss 0.5320, test accuracy 0.7396\n",
      "Epoch 3, train loss 0.5233, train acc 0.7433, test loss 0.5227, test accuracy 0.7436\n",
      "Epoch 4, train loss 0.5211, train acc 0.7418, test loss 0.5207, test accuracy 0.7420\n",
      "Epoch 5, train loss 0.5114, train acc 0.7482, test loss 0.5111, test accuracy 0.7484\n",
      "Epoch 6, train loss 0.5032, train acc 0.7596, test loss 0.5030, test accuracy 0.7597\n",
      "Epoch 7, train loss 0.4986, train acc 0.7642, test loss 0.4985, test accuracy 0.7643\n",
      "Epoch 8, train loss 0.4979, train acc 0.7649, test loss 0.4979, test accuracy 0.7650\n",
      "Epoch 9, train loss 0.4981, train acc 0.7645, test loss 0.4981, test accuracy 0.7646\n",
      "Epoch 10, train loss 0.4959, train acc 0.7655, test loss 0.4959, test accuracy 0.7656\n",
      "Epoch 11, train loss 0.4927, train acc 0.7675, test loss 0.4927, test accuracy 0.7676\n",
      "Epoch 12, train loss 0.4892, train acc 0.7695, test loss 0.4892, test accuracy 0.7696\n",
      "Epoch 13, train loss 0.4859, train acc 0.7717, test loss 0.4860, test accuracy 0.7717\n",
      "Epoch 14, train loss 0.4830, train acc 0.7737, test loss 0.4830, test accuracy 0.7736\n",
      "Epoch 15, train loss 0.4803, train acc 0.7750, test loss 0.4804, test accuracy 0.7750\n",
      "Epoch 16, train loss 0.4778, train acc 0.7764, test loss 0.4779, test accuracy 0.7763\n",
      "Epoch 17, train loss 0.4756, train acc 0.7773, test loss 0.4757, test accuracy 0.7772\n",
      "Epoch 18, train loss 0.4736, train acc 0.7787, test loss 0.4737, test accuracy 0.7786\n",
      "Epoch 19, train loss 0.4718, train acc 0.7794, test loss 0.4719, test accuracy 0.7793\n",
      "Epoch 20, train loss 0.4702, train acc 0.7800, test loss 0.4703, test accuracy 0.7798\n",
      "Epoch 21, train loss 0.4687, train acc 0.7806, test loss 0.4688, test accuracy 0.7805\n",
      "Epoch 22, train loss 0.4674, train acc 0.7810, test loss 0.4675, test accuracy 0.7809\n",
      "Epoch 23, train loss 0.4661, train acc 0.7816, test loss 0.4662, test accuracy 0.7816\n",
      "Epoch 24, train loss 0.4649, train acc 0.7825, test loss 0.4650, test accuracy 0.7824\n",
      "Epoch 25, train loss 0.4638, train acc 0.7829, test loss 0.4639, test accuracy 0.7828\n",
      "Epoch 26, train loss 0.4627, train acc 0.7834, test loss 0.4628, test accuracy 0.7833\n",
      "Epoch 27, train loss 0.4616, train acc 0.7840, test loss 0.4617, test accuracy 0.7838\n",
      "Epoch 28, train loss 0.4606, train acc 0.7843, test loss 0.4607, test accuracy 0.7842\n",
      "Epoch 29, train loss 0.4597, train acc 0.7847, test loss 0.4598, test accuracy 0.7846\n",
      "Epoch 30, train loss 0.4588, train acc 0.7851, test loss 0.4589, test accuracy 0.7850\n",
      "Epoch 31, train loss 0.4579, train acc 0.7857, test loss 0.4580, test accuracy 0.7856\n",
      "Epoch 32, train loss 0.4570, train acc 0.7864, test loss 0.4571, test accuracy 0.7862\n",
      "Epoch 33, train loss 0.4562, train acc 0.7866, test loss 0.4563, test accuracy 0.7865\n",
      "Epoch 34, train loss 0.4554, train acc 0.7871, test loss 0.4555, test accuracy 0.7870\n",
      "Epoch 35, train loss 0.4546, train acc 0.7872, test loss 0.4547, test accuracy 0.7871\n",
      "Epoch 36, train loss 0.4538, train acc 0.7877, test loss 0.4539, test accuracy 0.7876\n",
      "Epoch 37, train loss 0.4531, train acc 0.7883, test loss 0.4532, test accuracy 0.7882\n",
      "Epoch 38, train loss 0.4523, train acc 0.7887, test loss 0.4524, test accuracy 0.7886\n",
      "Epoch 39, train loss 0.4516, train acc 0.7893, test loss 0.4517, test accuracy 0.7893\n",
      "Epoch 40, train loss 0.4509, train acc 0.7897, test loss 0.4510, test accuracy 0.7896\n",
      "Epoch 41, train loss 0.4502, train acc 0.7902, test loss 0.4503, test accuracy 0.7901\n",
      "Epoch 42, train loss 0.4495, train acc 0.7908, test loss 0.4496, test accuracy 0.7907\n",
      "Epoch 43, train loss 0.4489, train acc 0.7912, test loss 0.4490, test accuracy 0.7911\n",
      "Epoch 44, train loss 0.4483, train acc 0.7915, test loss 0.4484, test accuracy 0.7914\n",
      "Epoch 45, train loss 0.4477, train acc 0.7918, test loss 0.4478, test accuracy 0.7918\n",
      "Epoch 46, train loss 0.4471, train acc 0.7921, test loss 0.4472, test accuracy 0.7920\n",
      "Epoch 47, train loss 0.4466, train acc 0.7923, test loss 0.4467, test accuracy 0.7922\n",
      "Epoch 48, train loss 0.4461, train acc 0.7924, test loss 0.4462, test accuracy 0.7923\n",
      "Epoch 49, train loss 0.4456, train acc 0.7927, test loss 0.4457, test accuracy 0.7926\n",
      "Epoch 50, train loss 0.4451, train acc 0.7928, test loss 0.4452, test accuracy 0.7928\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for batch in batches:\n",
    "            names, labels = batch\n",
    "\n",
    "            sess.run([train_], {\n",
    "                input_: names,\n",
    "                labels_: labels})\n",
    "\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        for mini_batch in batches:\n",
    "            names, labels = mini_batch\n",
    "            mini_loss, mini_acc = evaluate_LSTM(sess, loss_, classify_, names, labels)\n",
    "            train_loss += mini_loss\n",
    "            train_acc += mini_acc\n",
    "\n",
    "        train_loss = train_loss/len(batches)\n",
    "        train_acc = train_acc/len(batches)\n",
    "        pl_loss[e,0] = train_loss\n",
    "        pl_acc[e,0] = train_acc\n",
    "        \n",
    "        # Performance on the test set\n",
    "        test_loss, test_acc = evaluate_LSTM(sess, loss_, classify_, test_data, test_labels)\n",
    "        pl_loss[e,1] = test_loss\n",
    "        pl_acc[e,1] = test_acc\n",
    "        \n",
    "        print(\"Epoch {:d}, train loss {:.4f}, train acc {:.4f}, test loss {:.4f}, test accuracy {:.4f}\".format(e+1, train_loss[0], train_acc, test_loss[0], test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Multi-layer perceptron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_NN_model(emb_size, vocab_size, T, learning_rate=0.001):\n",
    "\n",
    "    pad_vector = tf.zeros(shape=(1, emb_size), dtype=tf.float32, name=\"zero_padding\")\n",
    "    symbol_embedding = tf.get_variable('symbol_embeddings', shape=(vocab_size, emb_size), dtype=tf.float32)\n",
    "\n",
    "    symbol_embedding = tf.concat([pad_vector, symbol_embedding], axis=0)\n",
    "\n",
    "    input_ = tf.placeholder(shape=[None, T], dtype=tf.int32)\n",
    "    labels_ = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "\n",
    "    embedded = tf.nn.embedding_lookup(symbol_embedding, input_)\n",
    "\n",
    "    layer_1 = tf.keras.layers.Dense(13,activation=tf.nn.leaky_relu)(embedded)\n",
    "    layer_2 = tf.keras.layers.Dense(7,activation=tf.nn.relu)(layer_1)\n",
    "\n",
    "    output = tf.keras.layers.Flatten()(layer_2)\n",
    "    logits = tf.keras.layers.Dense(1)(output)\n",
    "\n",
    "    classify = tf.nn.sigmoid(logits)\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels_), axis=0)\n",
    "\n",
    "    train = tf.contrib.opt.LazyAdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'train': train,\n",
    "        'input': input_,\n",
    "        'labels': labels_,\n",
    "        'loss': loss,\n",
    "        'classify': classify\n",
    "    }\n",
    "\n",
    "def evaluate_NN(tf_session, tf_loss, tf_classify, data, labels):\n",
    "\n",
    "    loss_val, predict = tf_session.run([tf_loss, tf_classify], {\n",
    "        input_: data,\n",
    "        labels_: labels\n",
    "    })\n",
    "    acc_val = accuracy_score(labels, np.where(predict > 0.5, 1, 0))\n",
    "\n",
    "    return loss_val, acc_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify MLP Specific training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "max_len = p_train_data['Name'].str.len().max()\n",
    "train_data, train_labels, voc = transform_data(train, max_len)\n",
    "test_data, test_labels, _ = transform_data(test, max_len)\n",
    "batches = get_minibatches(train_data, train_labels, minibatch_size)\n",
    "terminals = create_NN_model(letter_embedding_size, len(voc),max_len)\n",
    "\n",
    "train_ = terminals['train']\n",
    "input_ = terminals['input']\n",
    "labels_ = terminals['labels']\n",
    "loss_ = terminals['loss']\n",
    "classify_ = terminals['classify']\n",
    "\n",
    "pl_loss = np.zeros((epochs,2))\n",
    "pl_acc = np.zeros((epochs,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluating MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss 0.6146, train acc 0.6658, test loss 0.6139, test accuracy 0.6664\n",
      "Epoch 1, train loss 0.6068, train acc 0.6719, test loss 0.6060, test accuracy 0.6722\n",
      "Epoch 2, train loss 0.5876, train acc 0.6789, test loss 0.5869, test accuracy 0.6791\n",
      "Epoch 3, train loss 0.5720, train acc 0.6879, test loss 0.5712, test accuracy 0.6880\n",
      "Epoch 4, train loss 0.5634, train acc 0.6947, test loss 0.5627, test accuracy 0.6948\n",
      "Epoch 5, train loss 0.5547, train acc 0.7019, test loss 0.5540, test accuracy 0.7019\n",
      "Epoch 6, train loss 0.5499, train acc 0.7062, test loss 0.5493, test accuracy 0.7062\n",
      "Epoch 7, train loss 0.5461, train acc 0.7106, test loss 0.5456, test accuracy 0.7107\n",
      "Epoch 8, train loss 0.5411, train acc 0.7142, test loss 0.5406, test accuracy 0.7143\n",
      "Epoch 9, train loss 0.5392, train acc 0.7167, test loss 0.5388, test accuracy 0.7168\n",
      "Epoch 10, train loss 0.5376, train acc 0.7193, test loss 0.5372, test accuracy 0.7194\n",
      "Epoch 11, train loss 0.5317, train acc 0.7246, test loss 0.5312, test accuracy 0.7248\n",
      "Epoch 12, train loss 0.5325, train acc 0.7258, test loss 0.5321, test accuracy 0.7260\n",
      "Epoch 13, train loss 0.5286, train acc 0.7291, test loss 0.5282, test accuracy 0.7293\n",
      "Epoch 14, train loss 0.5310, train acc 0.7280, test loss 0.5306, test accuracy 0.7282\n",
      "Epoch 15, train loss 0.5272, train acc 0.7308, test loss 0.5268, test accuracy 0.7309\n",
      "Epoch 16, train loss 0.5258, train acc 0.7335, test loss 0.5254, test accuracy 0.7336\n",
      "Epoch 17, train loss 0.5258, train acc 0.7336, test loss 0.5254, test accuracy 0.7338\n",
      "Epoch 18, train loss 0.5280, train acc 0.7322, test loss 0.5276, test accuracy 0.7324\n",
      "Epoch 19, train loss 0.5251, train acc 0.7340, test loss 0.5246, test accuracy 0.7341\n",
      "Epoch 20, train loss 0.5256, train acc 0.7347, test loss 0.5251, test accuracy 0.7349\n",
      "Epoch 21, train loss 0.5254, train acc 0.7344, test loss 0.5249, test accuracy 0.7345\n",
      "Epoch 22, train loss 0.5254, train acc 0.7362, test loss 0.5250, test accuracy 0.7363\n",
      "Epoch 23, train loss 0.5241, train acc 0.7350, test loss 0.5237, test accuracy 0.7351\n",
      "Epoch 24, train loss 0.5249, train acc 0.7362, test loss 0.5245, test accuracy 0.7364\n",
      "Epoch 25, train loss 0.5237, train acc 0.7361, test loss 0.5233, test accuracy 0.7362\n",
      "Epoch 26, train loss 0.5246, train acc 0.7377, test loss 0.5242, test accuracy 0.7379\n",
      "Epoch 27, train loss 0.5236, train acc 0.7374, test loss 0.5231, test accuracy 0.7375\n",
      "Epoch 28, train loss 0.5249, train acc 0.7379, test loss 0.5245, test accuracy 0.7381\n",
      "Epoch 29, train loss 0.5241, train acc 0.7367, test loss 0.5236, test accuracy 0.7368\n",
      "Epoch 30, train loss 0.5250, train acc 0.7375, test loss 0.5246, test accuracy 0.7376\n",
      "Epoch 31, train loss 0.5239, train acc 0.7366, test loss 0.5235, test accuracy 0.7367\n",
      "Epoch 32, train loss 0.5251, train acc 0.7374, test loss 0.5247, test accuracy 0.7375\n",
      "Epoch 33, train loss 0.5235, train acc 0.7375, test loss 0.5231, test accuracy 0.7376\n",
      "Epoch 34, train loss 0.5246, train acc 0.7366, test loss 0.5242, test accuracy 0.7367\n",
      "Epoch 35, train loss 0.5242, train acc 0.7362, test loss 0.5238, test accuracy 0.7364\n",
      "Epoch 36, train loss 0.5250, train acc 0.7365, test loss 0.5246, test accuracy 0.7366\n",
      "Epoch 37, train loss 0.5246, train acc 0.7364, test loss 0.5242, test accuracy 0.7366\n",
      "Epoch 38, train loss 0.5242, train acc 0.7369, test loss 0.5238, test accuracy 0.7371\n",
      "Epoch 39, train loss 0.5237, train acc 0.7371, test loss 0.5233, test accuracy 0.7372\n",
      "Epoch 40, train loss 0.5252, train acc 0.7341, test loss 0.5249, test accuracy 0.7342\n",
      "Epoch 41, train loss 0.5234, train acc 0.7371, test loss 0.5231, test accuracy 0.7372\n",
      "Epoch 42, train loss 0.5233, train acc 0.7375, test loss 0.5230, test accuracy 0.7376\n",
      "Epoch 43, train loss 0.5227, train acc 0.7380, test loss 0.5224, test accuracy 0.7381\n",
      "Epoch 44, train loss 0.5229, train acc 0.7382, test loss 0.5226, test accuracy 0.7383\n",
      "Epoch 45, train loss 0.5225, train acc 0.7388, test loss 0.5222, test accuracy 0.7389\n",
      "Epoch 46, train loss 0.5224, train acc 0.7390, test loss 0.5221, test accuracy 0.7392\n",
      "Epoch 47, train loss 0.5224, train acc 0.7391, test loss 0.5221, test accuracy 0.7392\n",
      "Epoch 48, train loss 0.5223, train acc 0.7393, test loss 0.5221, test accuracy 0.7394\n",
      "Epoch 49, train loss 0.5223, train acc 0.7393, test loss 0.5220, test accuracy 0.7394\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for batch in batches:\n",
    "            names, labels = batch\n",
    "\n",
    "            sess.run([train_], {\n",
    "                input_: names,\n",
    "                labels_: labels\n",
    "            })\n",
    "            \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        for mini_batch in batches:\n",
    "            names, labels = mini_batch\n",
    "            mini_loss, mini_acc = evaluate_NN(sess, loss_, classify_, names, labels)\n",
    "            train_loss += mini_loss\n",
    "            train_acc += mini_acc\n",
    "\n",
    "        train_loss = train_loss/len(batches)\n",
    "        train_acc = train_acc/len(batches)\n",
    "        pl_loss[e,0] = train_loss\n",
    "        pl_acc[e,0] = train_acc\n",
    "\n",
    "        # Performance on the test set\n",
    "        test_loss, test_acc = evaluate_NN(sess, loss_, classify_, test_data, test_labels)\n",
    "        pl_loss[e,1] = test_loss\n",
    "        pl_acc[e,1] = test_acc\n",
    "        \n",
    "        print(\"Epoch {:d}, train loss {:.4f}, train acc {:.4f}, test loss {:.4f}, test accuracy {:.4f}\".format(e, train_loss[0], train_acc, test_loss[0], test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Conclusion <center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
